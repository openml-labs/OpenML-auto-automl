{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OpenML Auto-AutoML","text":"<ul> <li>This repository is under a project called AutoML4All, an initiative to provide AutoML tools to anyone without programming experience.</li> <li>At the moment, this is focused on only Tabular data.</li> <li>If anyone uploads a dataset to OpenML, we perform these steps every n (here 5) hours<ul> <li>Check if there are new datasets based on what was previously stored</li> <li>Identify if there is a Task, if not, then try to create one based on the target variable and data type of the target column.</li> <li>Once a task is created, summon amlb and based on the chosen automl frameworks, send a request to the GPU servers</li> <li>For now, we are using Snellius</li> <li>Once the frameworks are done running, upload the run results back to OpenML.</li> </ul> </li> </ul>"},{"location":"#how-to-use","title":"How to use?","text":"<ul> <li>If this project has not shut down, you should just be able to access it by uploading a dataset to OpenML and checking back in a few hours.</li> </ul>"},{"location":"#developer-configuration","title":"Developer configuration","text":"<ul> <li>Hello future OpenML developer, so you want to re-run/make a new version of this? Just look at the rest of this documentation</li> </ul>"},{"location":"contribution/","title":"Contributing","text":"<ul> <li>Just like any other open source project contributions are always welcome. If you have any ideas whatsoever, just file an issue on Github, and we can discuss if it makes sense to implement it.</li> <li>Since a large part of this project depends more on amlb, in all   honesty, it makes sense to focus contributions there more than this particular library.</li> </ul>"},{"location":"limitations/","title":"Limitations -&gt; Future Work","text":"<ul> <li>So obviously, there are a few limitations of this library. These are listed here for future reference.</li> </ul>"},{"location":"limitations/#snellius","title":"Snellius","text":"<ul> <li>Using Snellius complicates this a lot since they dont' allow for cron jobs. But until we   get our own GPU server, this has to suffice.</li> </ul>"},{"location":"limitations/#dataset-limitations","title":"Dataset Limitations","text":"<ul> <li>Sometimes new data sets that are uploaded do not specify the correct target variable or   have some metadata missing that does not allow us to automatically decide what to do with it. Since there is currently no way to deal with that, we just ignore running the framework for these datasets.</li> </ul>"},{"location":"limitations/#multimodal","title":"MultiModal","text":"<ul> <li>OpenML Currently does not have a tag that mentions what kind of data the current data set is. For instance, if there is an image data set open ML currently does not upload all the images directly and so what we get as a header file. As you can imagine, though, running an auto ML pipeline on such a file is pointless. But at the moment there is no way for us to detect it so we ignore that.</li> <li>Due to this, we also do not support specific types of tasks like time series prediction, for example since there is currently no way for us to know if the dataset being processed belongs to one of these categories.</li> </ul>"},{"location":"limitations/#framework-limitations","title":"Framework Limitations","text":"<ul> <li>Since most of the auto ML frameworks that we use in this library are externally managed,We do not control how frequently they are updated. This means that sometimes frameworks just straight out do not work maybe because they are out of date or they are no longer maintained, etc.. While we will do our best to make sure something like this does not happen, sometimes it is a bit unpredictable.</li> <li>For example at the time of writing this documentation, AutoGluon no longer supports OpenML. The developers are of course working to fix this, but we do not know how long it will take.</li> </ul>"},{"location":"Setup/","title":"Setup Overview","text":"<ul> <li>The setup here is a little complicated due to the limitations of not having our own GPU server (for now).</li> <li>Since we don't have our own GPUs (yet), we are using Snellius to run the automl pipelines. Now, Snellius does not really want us to do that, hence not allowing for cron jobs. To overcome this, we use the OpenML test server.</li> </ul>"},{"location":"Setup/#components","title":"Components","text":"<ul> <li>Therefore to deploy the solution, we need to configure multiple things in multiple places. This will be explained in this section.</li> <li>Look at snellius first, then test server</li> </ul>"},{"location":"Setup/openml_test_server/","title":"Setup for Openml Test Server","text":"<ul> <li>The main reason we need this is because Snellius does not allow for cron jobs.</li> <li>If we eventually find a better solution that is not Snellius, we should eventually migrate to a different platform. This is more of a temporary solution than a permanent one.</li> </ul>"},{"location":"Setup/openml_test_server/#steps-to-follow","title":"Steps to follow","text":"<ul> <li>The first thing to do is authenticate using ssh to snellius from the test server.</li> <li>Turn on your VPN, ssh into the test server using your credentials <code>ssh user@openml-test.win.tue.nl</code></li> <li>I would switch to bash at this point by just typing <code>bash</code></li> <li>Now for the slightly complicated part, we shall authenticate using ssh the credentials for Snellius here. Note that you should avoid using a password when it asks you to create one. (YES THIS IS UNSAFE. We really need to find a better solution)<ul> <li>Check if the ssh agent is running <code>eval \"$(ssh-agent -s)\"</code></li> <li>Generate an SSH key to connect to the server. Give it no password <code>ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_rsa -N \"\"</code></li> <li>Now copy the SSH key over to Snellius <code>ssh-copy-id -i ~/.ssh/id_rsa.pub user@snellius.surf.nl</code></li> <li>See if this works without a password now <code>ssh 'user@snellius.surf.nl'</code></li> <li>Add the SSH to your SSH manager <code>ssh-add ~/.ssh/id_rsa</code></li> </ul> </li> <li>Once that is done, we set up a cron job to run the script every n hours (here 5)<ul> <li>This assumes a few things<ul> <li>On Snellius, you have cloned this repository in <code>/home/user/</code></li> <li>You have an api key from OpenML</li> </ul> </li> <li>Type <code>crontab -e</code> and paste this line at the end<ul> <li><code>0 */5 * * * ssh -i /home/user/.ssh/id_rsa user@snellius.surf.nl \"bash -c '/home/user/OpenML-auto-automl/snellius_env_loader_cron.sh &amp;&amp; python /home/user/OpenML-auto-automl/src/snellius_generate_sbatch.py -c -a apikey'\" &gt;&gt; /home/user/cron_log.txt 2&gt;&amp;1</code></li> </ul> </li> <li>The logs are stored in <code>/home/user/cron_log.txt</code></li> </ul> </li> </ul>"},{"location":"Setup/snellius/","title":"Snellius Setup","text":"<ul> <li>Now for Snellius</li> <li>As always, first ssh into Snellius <code>ssh user@snellius.surf.nl</code></li> <li>This one is pretty involved, so make sure you have the time for it.</li> </ul>"},{"location":"Setup/snellius/#steps","title":"Steps","text":"<ul> <li>We need to first log into Github of course, if you haven't done so already run <code>git config --global credential.helper cache</code> and then enter your details. Note that the password is your access token. You can always use another authentication if you like.<ul> <li>I prefer to use the github command tool <code>gh</code> but we cannot install it here.</li> </ul> </li> <li>Once that is done, go to your home directory and clone these repositories<ul> <li><code>git clone https://github.com/openml-labs/OpenML-auto-automl</code></li> <li><code>git clone https://github.com/SubhadityaMukherjee/automlbenchmark</code></li> </ul> </li> <li>In your home directory, do a <code>mkdir automl_data</code> . This should run automatically when</li> <li>Now just load Python and it's required modules so we can do the rest of the setup. You can do this using <code>cd /home/user/OpenML-auto-automl &amp;&amp; ./snellius_env_loader.sh</code> . If you already built a conda environment, feel free to skip recreating it. you execute it but just in case :P</li> <li>Now we need to build the Singularity (It is a docker alternative. Snellius only supports   open source software so we use it) images for all the frameworks we want to run.<ul> <li>Since all of this is on the server, I prefer using <code>nvim</code> as my editor of choice. But if you are using VSCode or some such, feel free to just open it however you want.</li> <li>cd to <code>automlbenchmark/scripts/</code></li> <li><code>nohup python build_images.py -m singularity -f autosklearn,flaml,gama &gt; build_log.txt 2&gt;&amp;1 &amp;</code></li> <li>Of course, add whatever other frameworks you want to add.</li> <li>If this does not work by any chance, you can also use <code>cd /home/user/OpenML-auto-automl &amp;&amp; nohup ./build_images.sh user singularity &gt; build_log.txt 2&gt;&amp;1 &amp;</code></li> <li>This will take A LONG TIME to run and so the logs are stored in <code>build_log.txt</code></li> <li>Note that NOTHING else will work until these are built.</li> </ul> </li> <li>Alright! Now that all of this is done (hopefully) you should be good to go.</li> <li>Do you want to set up the cron job? Move to test server docs</li> <li>Do you want to manually check if it is working? <code>cd /home/user/OpenML-auto-automl/src &amp;&amp; python snellius_generate_sbatch.py -c -a apikey</code></li> <li>Results are stored in <code>/home/automl_data</code></li> </ul>"},{"location":"Setup/snellius/#faqs","title":"FAQs","text":"<ul> <li>How do I check if new datasets are being downloaded?<ul> <li>Go to <code>/home/user/automl_data</code> and check the file <code>dataset_list_for_cronjob.csv</code>. If the   first few rows do not correspond to new data, something is wrong</li> </ul> </li> <li>How do I know if the pipeline is working?<ul> <li>New dataset on OpenML -&gt; New task + run on OpenML</li> <li>There should be new slurm files in <code>/home/user/automl_data</code></li> </ul> </li> </ul>"}]}